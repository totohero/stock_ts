{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "datalore": {
          "hide_input_from_viewers": true,
          "hide_output_from_viewers": true,
          "node_id": "0gqj5emNsKNyioPsiMDg1k",
          "type": "MD"
        },
        "id": "SB7D5AqOOxXy"
      },
      "source": [
        "# Connect to MongoDb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFgGICG_PPlQ",
        "outputId": "b49d8513-e889-44a4-83d8-6fd869efe6f8"
      },
      "outputs": [],
      "source": [
        "# %pip install pymongo pykrx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_GIOKhyPe2j",
        "outputId": "9ed69940-ff33-43d9-ee83-be40fcd7e717"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: MONGO_URI=mongodb+srv://totohero:86nggolxqPg2kC8G@cluster0-seoul-1st.coz7epy.mongodb.net/?retryWrites=true&w=majority\n",
            "env: START_DATE=2023-01-01\n",
            "env: END_DATE=2023-02-07\n"
          ]
        }
      ],
      "source": [
        "%env MONGO_URI=mongodb+srv://totohero:86nggolxqPg2kC8G@cluster0-seoul-1st.coz7epy.mongodb.net/?retryWrites=true&w=majority\n",
        "%env START_DATE=2023-01-01\n",
        "%env END_DATE=2023-02-07"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "datalore": {
          "hide_input_from_viewers": true,
          "hide_output_from_viewers": true,
          "node_id": "3vCdCXkzMRM49DbDD7HdQc",
          "type": "MD"
        },
        "id": "AYbV0k2VOxX1"
      },
      "source": [
        "# MongoDb setup\n",
        "## Define collections\n",
        "- meta\n",
        "- date_collection\n",
        "- stock_ts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "datalore": {
          "hide_input_from_viewers": true,
          "hide_output_from_viewers": true,
          "node_id": "TsfjWBmz6OH5ZjvKxF3g0x",
          "type": "CODE"
        },
        "id": "LolokRq1OxX1",
        "outputId": "d59984ed-8297-4839-f599-b56d584f5ef8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pinged your deployment. You successfully connected to MongoDB!\n"
          ]
        }
      ],
      "source": [
        "from pymongo.mongo_client import MongoClient\n",
        "from pymongo.server_api import ServerApi\n",
        "import os\n",
        "\n",
        "# 86nggolxqPg2kC8G\n",
        "uri = os.environ[\"MONGO_URI\"]\n",
        "# Create a new client and connect to the server\n",
        "client = MongoClient(uri, server_api=ServerApi('1'))\n",
        "# Send a ping to confirm a successful connection\n",
        "try:\n",
        "    client.admin.command('ping')\n",
        "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "\n",
        "db = client['stock_db']\n",
        "\n",
        "meta = db['meta']\n",
        "date_collection = db['date']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "datalore": {
          "hide_input_from_viewers": true,
          "hide_output_from_viewers": true,
          "node_id": "yAbFvFGUucZVHhfOIi7SD5",
          "type": "MD"
        },
        "id": "_UKzBcaTOxX3"
      },
      "source": [
        "* stock_db\n",
        "    * meta collection\n",
        "        * { 'name' : 'ticker_synced_dates', 'dates' : array of dates } ticker 취합한 모든 날짜들\n",
        "        * { 'name' : 'ohlcv_synced_dates', 'symbol_dates' : array of {'symbol' : symbol, 'begin' : begin, 'end' : end}}\n",
        "        * { 'name' : 'ticker_set', 'tickers' : array of tickers } 존재했던 모든 ticker들\n",
        "    * date collection (starting from START_DATE)\n",
        "        * { 'date' : date, 'tickers' : 날짜별 모든 ticker들 }\n",
        "    * stock_ts collection (timeseries)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "datalore": {
          "hide_input_from_viewers": true,
          "hide_output_from_viewers": true,
          "node_id": "okmTp4tuSov6ruzvGbtqbX",
          "type": "MD"
        },
        "id": "ERUshP8HOxX4"
      },
      "source": [
        "# Crawl tickers\n",
        "- ticker_synced_dates: set of dates where ticker of that day has been collected\n",
        "- iterate from START_DATE to END_DATE\n",
        "    - if date is NOT in **ticker_synced_dates**\n",
        "        - fetch ticker of the day and update to **date_collection**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "datalore": {
          "hide_input_from_viewers": true,
          "hide_output_from_viewers": true,
          "node_id": "A82Zs0OzfbeXQ2RaRKqC52",
          "type": "CODE"
        },
        "id": "-W7qeuB5OxX4",
        "outputId": "b6305558-cf0c-4a7d-f549-0a667a30b9ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skipping 2023-01-02\n",
            "Skipping 2023-01-03\n",
            "Skipping 2023-01-04\n",
            "Skipping 2023-01-05\n",
            "Skipping 2023-01-06\n",
            "Skipping 2023-01-09\n",
            "Skipping 2023-01-10\n",
            "Skipping 2023-01-11\n",
            "Skipping 2023-01-12\n",
            "Skipping 2023-01-13\n",
            "Skipping 2023-01-16\n",
            "Skipping 2023-01-17\n",
            "Skipping 2023-01-18\n",
            "Skipping 2023-01-19\n",
            "Skipping 2023-01-20\n",
            "Skipping 2023-01-23\n",
            "Skipping 2023-01-24\n",
            "Skipping 2023-01-25\n",
            "Skipping 2023-01-26\n",
            "Skipping 2023-01-27\n",
            "Skipping 2023-01-30\n",
            "Skipping 2023-01-31\n",
            "Skipping 2023-02-01\n",
            "Skipping 2023-02-02\n",
            "Skipping 2023-02-03\n",
            "Skipping 2023-02-06\n",
            "Skipping 2023-02-07\n",
            "Done\n"
          ]
        }
      ],
      "source": [
        "# Exhaustive crawling of tickers per day\n",
        "# Very time-consuming. Use only if necessary\n",
        "\n",
        "from pykrx import stock\n",
        "import time\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# 자료 수집 시작일\n",
        "global_begin_date = datetime.strptime(os.environ['START_DATE'], '%Y-%m-%d')\n",
        "\n",
        "# 자료 수집 종료일\n",
        "try:\n",
        "    global_end_date = datetime.strptime(os.environ['END_DATE'], '%Y-%m-%d')\n",
        "except (TypeError, KeyError):\n",
        "    global_end_date = datetime.today()\n",
        "\n",
        "# 일자별 자료 수집 여부 확인\n",
        "try:\n",
        "    ticker_synced_dates = meta.find_one({'name': 'ticker_synced_dates'})['dates']\n",
        "except (TypeError, KeyError):\n",
        "    ticker_synced_dates = []\n",
        "\n",
        "# 자료 수집 안된 날에 한해 해당일의 모든 ticker list 수집\n",
        "dt = pd.date_range(start=global_begin_date, end=global_end_date, freq='B')\n",
        "for d in dt:\n",
        "    curr_date = d.date()\n",
        "    curr_datetime = datetime(year=curr_date.year, month=curr_date.month, day=curr_date.day)\n",
        "\n",
        "    if curr_datetime in ticker_synced_dates:\n",
        "        print(\"Skipping \" + curr_date.strftime('%Y-%m-%d'))\n",
        "    else:\n",
        "        print(\"Fetching tickers on \" + curr_date.strftime('%Y-%m-%d'))\n",
        "        tickers = stock.get_market_ticker_list(date=curr_date, market=\"ALL\")\n",
        "        date_collection.update_one({'date': curr_datetime}, {'$set': {'tickers': tickers}}, upsert=True)\n",
        "        meta.update_one({'name': 'ticker_synced_dates'}, {'$push': {'dates': curr_datetime}}, upsert=True)\n",
        "\n",
        "        # meta ticker_set은 날짜와 무관하게 존재했던 모든 ticker들의 집합\n",
        "        meta.update_one({'name': 'ticker_set'}, {'$addToSet': {'tickers': {'$each': tickers}}}, upsert=True)\n",
        "        time.sleep(0.1)\n",
        "print(\"Done\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "datalore": {
          "hide_input_from_viewers": true,
          "hide_output_from_viewers": true,
          "node_id": "vrlWPZGjdBAYEC46eY8yPO",
          "type": "MD"
        },
        "id": "0joJbR1wOxX5"
      },
      "source": [
        "# Crawl time-series of each symbol"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "datalore": {
          "hide_input_from_viewers": true,
          "hide_output_from_viewers": true,
          "node_id": "B9Ulh2y9IgMobVpRfivOdw",
          "type": "CODE"
        },
        "id": "zj9fwNbOOxX6"
      },
      "outputs": [],
      "source": [
        "from datetime import timedelta\n",
        "\n",
        "def missing_dates(prev_begin: datetime, prev_end: datetime, curr_begin: datetime, curr_end: datetime) -> list[datetime]:\n",
        "    '''\n",
        "    This is test:\n",
        "    >>> from datetime import datetime\n",
        "    >>> prev_begin = datetime(2020, 1, 1)\n",
        "    >>> prev_end = datetime(2020, 1, 10)\n",
        "    >>> curr_begin = datetime(2020, 1, 5)\n",
        "    >>> curr_end = datetime(2020, 1, 15)\n",
        "    >>> missing_dates(prev_begin, prev_end, curr_begin, curr_end)\n",
        "    [(datetime.datetime(2020, 1, 11, 0, 0), datetime.datetime(2020, 1, 15, 0, 0))]\n",
        "    '''\n",
        "    if prev_end < curr_begin or curr_end < prev_begin:\n",
        "        return [(curr_begin, curr_end)]\n",
        "    missing = []\n",
        "    if curr_begin < prev_begin:\n",
        "        missing.append((curr_begin, prev_begin-timedelta(days=1)))\n",
        "    if prev_end < curr_end:\n",
        "        missing.append((prev_end+timedelta(days=1), curr_end))\n",
        "    return missing\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "datalore": {
          "hide_input_from_viewers": true,
          "hide_output_from_viewers": true,
          "node_id": "ioFsgOIbwcEowK8zTHORPb",
          "type": "CODE"
        },
        "id": "hl9GBTQTOxX5",
        "outputId": "1610fc87-d582-49fc-e379-805780e79312"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Crawl from 2023-01-01 to 2023-02-07\n",
            "[(datetime.datetime(2023, 1, 1, 0, 0), datetime.datetime(2023, 2, 7, 0, 0))]\n",
            " Fetching OHLCV for 060310 (0) 2023-01-01 ~ 2023-02-07\n",
            "[(datetime.datetime(2023, 1, 1, 0, 0), datetime.datetime(2023, 2, 7, 0, 0))]\n",
            " Fetching OHLCV for 095570 (1) 2023-01-01 ~ 2023-02-07\n",
            "[(datetime.datetime(2023, 1, 1, 0, 0), datetime.datetime(2023, 2, 7, 0, 0))]\n",
            " Fetching OHLCV for 068400 (2) 2023-01-01 ~ 2023-02-07\n",
            "[(datetime.datetime(2023, 1, 1, 0, 0), datetime.datetime(2023, 2, 7, 0, 0))]\n",
            " Fetching OHLCV for 006840 (3) 2023-01-01 ~ 2023-02-07\n",
            "[(datetime.datetime(2023, 1, 1, 0, 0), datetime.datetime(2023, 2, 7, 0, 0))]\n",
            " Fetching OHLCV for 054620 (4) 2023-01-01 ~ 2023-02-07\n",
            "[]\n",
            "Done\n"
          ]
        }
      ],
      "source": [
        "# stock_ts가 없는 경우, 생성\n",
        "if 'stock_ts' not in db.list_collection_names():\n",
        "    db.create_collection('stock_ts', timeseries={'timeField': 'date', 'metaField': 'symbol',\n",
        "                                                 'granularity': 'hours'})\n",
        "\n",
        "stock_ts = db['stock_ts']  # 컬렉션(테이블) 선택\n",
        "\n",
        "\n",
        "def save_stock_ts(symbol, df):\n",
        "    # DataFrame을 MongoDB에 저장\n",
        "    df['symbol'] = symbol\n",
        "    records = df.to_dict(orient='records')\n",
        "    stock_ts.insert_many(records)\n",
        "\n",
        "\n",
        "# 역사상 존재했던 모든 ticker들의 집합\n",
        "tickers = meta.find_one({'name': 'ticker_set'})['tickers']\n",
        "\n",
        "# ticker별로 sync된 날짜들의 map\n",
        "try:\n",
        "    symbol_dates = meta.find_one({'name': 'ohlcv_synced_dates'})['symbol_dates']\n",
        "except (TypeError, KeyError):\n",
        "    symbol_dates = []\n",
        "\n",
        "\n",
        "def crawl_stock_by_date(ticker, begin_date, end_date):\n",
        "    df = stock.get_market_ohlcv_by_date(fromdate=begin_date, todate=end_date, ticker=ticker)\n",
        "    df = df.reset_index()\n",
        "    df = df.rename(\n",
        "            columns={'날짜': 'date', '시가': 'open', '고가': 'high', '저가': 'low', '종가': 'close', '거래량': 'volume',\n",
        "                     '거래대금': 'amount',\n",
        "                     '등락률': 'change'})\n",
        "    save_stock_ts(ticker, df)\n",
        "    time.sleep(0.1)\n",
        "\n",
        "def crawl_stock(begin_date:datetime, end_date:datetime):\n",
        "    print(\"Crawl from \" + begin_date.strftime('%Y-%m-%d') + \" to \" + end_date.strftime('%Y-%m-%d'))\n",
        "    for ind, ticker in enumerate(tickers):\n",
        "        try:\n",
        "            prev_sync = [sd for sd in symbol_dates if sd['ticker'] == ticker][0]\n",
        "            prev_sync_msg = \"previously synced from \" + prev_sync['begin'].strftime('%Y-%m-%d') + \" to \" + prev_sync['end'].strftime('%Y-%m-%d')\n",
        "            missing = missing_dates(prev_sync['begin'], prev_sync['end'], begin_date, end_date)\n",
        "            print(missing)\n",
        "        except:\n",
        "            prev_sync_msg = \"no previous sync\"\n",
        "            missing = [(begin_date, end_date)]\n",
        "        for (b,e) in missing:\n",
        "            print(\" Fetching OHLCV for \" + ticker + \" (\" + str(ind) + \") \" + b.strftime('%Y-%m-%d') + \" ~ \" + e.strftime('%Y-%m-%d'))\n",
        "            crawl_stock_by_date(ticker, b, e)\n",
        "        \n",
        "        meta.update_one({'name': 'ohlcv_synced_dates'},\n",
        "                        {'$addToSet': {'symbol_dates': {'ticker': ticker, 'begin': begin_date, 'end': end_date}}},\n",
        "                        upsert=True)\n",
        "        time.sleep(0.1)\n",
        "    print(\"Done\")\n",
        "\n",
        "\n",
        "# 일단 시작일, 종료일 기준 모두 수집\n",
        "crawl_stock(global_begin_date, global_end_date)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trying:\n",
            "    from datetime import datetime\n",
            "Expecting nothing\n",
            "ok\n",
            "Trying:\n",
            "    prev_begin = datetime(2020, 1, 1)\n",
            "Expecting nothing\n",
            "ok\n",
            "Trying:\n",
            "    prev_end = datetime(2020, 1, 10)\n",
            "Expecting nothing\n",
            "ok\n",
            "Trying:\n",
            "    curr_begin = datetime(2020, 1, 5)\n",
            "Expecting nothing\n",
            "ok\n",
            "Trying:\n",
            "    curr_end = datetime(2020, 1, 15)\n",
            "Expecting nothing\n",
            "ok\n",
            "Trying:\n",
            "    missing_dates(prev_begin, prev_end, curr_begin, curr_end)\n",
            "Expecting:\n",
            "    [(datetime.datetime(2020, 1, 11, 0, 0), datetime.datetime(2020, 1, 15, 0, 0))]\n",
            "ok\n",
            "4 items had no tests:\n",
            "    __main__\n",
            "    __main__.crawl_stock\n",
            "    __main__.crawl_stock_by_date\n",
            "    __main__.save_stock_ts\n",
            "1 items passed all tests:\n",
            "   6 tests in __main__.missing_dates\n",
            "6 tests in 5 items.\n",
            "6 passed and 0 failed.\n",
            "Test passed.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TestResults(failed=0, attempted=6)"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import doctest\n",
        "from datetime import datetime\n",
        "\n",
        "doctest.testmod(verbose=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "datalore": {
      "base_environment": "default",
      "computation_mode": "JUPYTER",
      "package_manager": "pip",
      "packages": [
        {
          "name": "pykrx",
          "source": "PIP",
          "version": "1.0.42"
        },
        {
          "name": "pymongo",
          "source": "PIP",
          "version": "4.4.0b0"
        }
      ],
      "report_row_ids": [],
      "version": 3
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
